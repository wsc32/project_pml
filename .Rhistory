boxplot(mpg~cyl, mtcars)
hist(mtcars$mpg)
set.seed(1)
rpois(5, 2)
?rpois
set.seed(1)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
?set.seed
set.seed(2)
rpois(5, 2)
set.seed(2)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
?rnorm
?rpois
rep(0:1, each = 5)
set.seed(10)
x <- rep(0:1, each = 5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
ploy(x,y)
plot(x,y)
?rpois
system.time()
library(swirl)
swirl()
head()
head()
head()
head()
head(x)
dir()
head(mydf)
q()
?strsplit
?names
?cnames
?less
?strsplit
?unique
?subset
library(swirl)
swirl()
swirl()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package=lubridate)
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day, label=TRUE)
this_moment <- now()
this_moment
hour(this_moment)
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
?wday
?qunif
qunif(0.75, 0, 1)
?qbeta
qbeta(0.5,1,2)
qbeta(0.5,2,1)
library(swirl)
swirl()
install_from_swirl("Statistical Inference")
pbinom(3,size=5,prob=0.5,lower.tail=FALSE)
ppois(10,lambda=5*4)
ppois(10,lambda=5*3)
?range
?seq
seq(from=20, to=40)
seq(from=20, to=40, length.out=122)
seq(from=20, to=40, length.out=12)
seq(from=0.1, to=0.4, length.out=8)
?matrix
matrix(nrow=8,ncol=12)
r <- seq(from=20, to=40, length.out=12)
c <- seq(from=0.1, to=0.4, length.out=8)
m <- matrix(nrow=8,ncol=12)
m <- r
m
m <- matrix(nrow=8,ncol=12)
m[n,]<-r
m[1,]<-r
m
r
c
c <- seq(from=20, to=40, length.out=12)
r <- seq(from=0.1, to=0.4, length.out=8)
clear
c
r
seq(from=25, to=35, length.out=12)
c
1100 +c(-1,1)*qt(0.95,8)*(30/3)
6/(qt(0.95,8))
qt(0.95,8)
6/1.856
-2 +c(-1,1)*qt(0.95,8)*(2.6/3)
-2 +c(-1,1)*qt(0.95,8)*(0.3/3)
-2 +c(-1,1)*qt(0.95,8)*(2.1/3)
-2 +c(-1,1)*qt(0.95,8)*(1.5/3)
sp <- sqrt((9*0.6^2+9*0.68^2)/(18))
3-5+c(-1,1)*qt(0.95,19)*sp*(1/10+1/10)^0.5
3-5+c(-1,1)*qt(0.95,18)*sp*(1/10+1/10)^0.5
x <- (0.6^2)/9
y <- (0.68^2)/9
df <- ((x+y)^2)/((x^2/9)+(y^2/9))
quantile(0.975,15.04)
quantile(15.05,0.975)
qt(0.95,15.04)
qt(0.975,15.04)
3-5+(c-1,1)*qt(0.95,df)*(x+y)^0.5
3-5+c(-1,1)*qt(0.95,df)*(x+y)^0.5
sqrt(7.92)
sp <- sqrt((9*0.6+9*0.68)/(18))
x <- (0.6)/9
y <- (0.68)/9
df <- ((x+y)^2)/((x^2/9)+(y^2/9))
3-5+(c-1,1)*qt(0.95,df)*(x+y)^0.5
3-5+c(-1,1)*qt(0.95,df)*(x+y)^0.5
sp <- sqrt((9*0.6+9*0.68)/(18))
3-5+c(-1,1)*qt(0.95,18)*sp*(1/10+1/10)^0.5
sp <- sqrt((9*0.6+9*0.68)/(18))
3-5+c(-1,1)*qt(0.975,18)*sp*(1/10+1/10)^0.5
6/(qt(0.975,8))
3-5+c(-1,1)*qt(0.95,18)*sp*(1/10+1/10)^0.5
?q
?qt
(6-4)+c(-1,1)*qnorm(0.975)*(0.5^2/100+2^2/100)^0.5
x <- 1.5^2/9
y <- 1.8^2/9
df <- (x+y)^2/(x^2/8+y^2/8)
df
-3-1+c(-1,1)*qt(0.95,df)*(x+y)^0.5
sp2 <- sqrt(9*1.5^2+9*1.8^2)/(16)
-3-1+c(-1,1)*qt(0.95,18)*sp2*(1/9+1/9)^0.5
sp2 <- sqrt((9*1.5^2+9*1.8^2)/(16))
-3-1+c(-1,1)*qt(0.95,18)*sp2*(1/9+1/9)^0.5
-3-1+c(-1,1)*qt(0.95,16)*sp2*(1/9+1/9)^0.5
install.packages("rattle")
library("rattle")
library("rattle")
library(rattle)
install.packages("rattle")
library(rattle)
library(rattle)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal(inTrain,)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case~., method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
library(rpart.plot)
install.packages(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
head(training)
trainn=ing$Class
training$Class
dim(training)
class(training)
names(training)
modFit <- train(class~., method="rpart",data=training)
set.seed(125)
modFit <- train(class~., method="rpart",data=training)
inTrain <- createDataPartition(y = segmentationOriginal$Case, p = 0.6,
list = FALSE)
training <- segmentationOriginal[inTrain, ]
testing <- segmentationOriginal[-inTrain, ]
set.seed(125)
modFit <- train(Class ~ ., method = "rpart", data = training)
fancyRpartPlot(modFit$finalModel)
library(pgmm)
install.packages("pgmm")
library(pgmm)
data(olive)
olive=olive[,-1]
modFit <- train(Area ~., method="rpart",data=olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata=newdata)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
names(trainSA)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data=trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,predict(modFit,newdata=trainSA))
missClass(testSA$chd,predict(modFit,newdata=testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
names(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
install.packages("randomForest")
library(randomForest)
modvowel <- randomForest(y ~ ., data = vowel.train)
head(modvowel)
print(modvowel)
order(varImp(modvowel),decreasing=T)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modvowel1 <- randomForest(y ~ ., data = vowel.train)
modvowel1 <- train(y ~., method="rf", data=vowel.train)
modvowel2 <- train(y ~., method="gbm", data=vowel.train)
install.packages("gbm")
install.packages("gbm")
install.packages("gbm")
modvowel2 <- train(y ~., method="gbm", data=vowel.train)
library(AppliedPredictiveModeling)
library(caret)
library(ElemStatLearn)
library(pgmm)
library(rpart)
library(gmb)
library(gbm)
library(lubridate)
library(forecast)
install.packages(forecast)
install.packages("forecast")
library(forecast)
library(e1071)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modvowel1 <- train(y ~., method="rf", data=vowel.train)
library(randomForest)
modvowel1 <- train(y ~., method="rf", data=vowel.train)
modvowel2 <- train(y ~., method="gbm", data=vowel.train)
pred1 <- predict(modvowel1,vowel.test)
pred2 <- predict(modvowel2,vowel.test)
confusionMatrix(pred1, vowel.test$y)
confusionMatrix(pred2, vowel.test$y)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
mod_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
pred_rf <- predict(mod_rf, vowel.test)
pred_gbm <- predict(mod_gbm, vowel.test)
confusionMatrix(pred_rf, vowel.test$y)$overall[1]
confusionMatrix(pred_gbm, vowel.test$y)$overall[1]
confusionMatrix(pred_1, vowel.test$y)$overall[1]
confusionMatrix(pred1, vowel.test$y)$overall[1]
confusionMatrix(pred2, vowel.test$y)$overall[1]
sum(pred_rf[predDF$pred_rf == predDF$pred_gbm] ==
predDF$y[predDF$pred_rf == predDF$pred_gbm]) /
sum(predDF$pred_rf == predDF$pred_gbm)
predDF <- data.frame(pred_rf, pred_gbm, y = vowel.test$y)
sum(pred_rf[predDF$pred_rf == predDF$pred_gbm] ==
predDF$y[predDF$pred_rf == predDF$pred_gbm]) /
sum(predDF$pred_rf == predDF$pred_gbm)
sum(pred_gbm[predDF$pred_rf == predDF$pred_gbm] ==
predDF$y[predDF$pred_rf == predDF$pred_gbm]) /
sum(predDF$pred_rf == predDF$pred_gbm)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
pred_rf <- train(y ~., method="rf", data=training)
names(training)
pred_rf <- train(diagnosis ~., method="rf", data=training)
pred_gbm <- train(diagnosis ~., method="gbm", data=training)
pred_lda <- train(diagnosis ~., method="lda", data=training)
pred_lda <- train(diagnosis ~., method="lda", data=training)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis=testing$diagnosis)
mod_rf <- pred_rf
mod_gbm <- pred_gbm
mod_lda <- pred_lda
pred_rf <- predict(mod_rf,testing)
pred_gbm <- predict(mod_gbm,testing)
pred_lda <- predict(mod_lda,testing)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~., method="rf", data=predDF)
combPred <- pedcit(combModFit, predDF)
combPred <- predit(combModFit, predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(pred_rf, testing$diagnosis)$overall[1]
confusionMatrix(pred_glm, testing$diagnosis)$overall[1]
confusionMatrix(pred_gbm, testing$diagnosis)$overall[1]
confusionMatrix(pred_lda, testing$diagnosis)$overall[1]
confusionMatrix(combPred, testing$diagnosis)$overall[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(3523)
library(elasticnet)
install.packages("elasticnet")
library(elasticnet)
set.seed(3523)
set.seed(233)
mod_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")
library(elasticnet)
plot.enet(mod_lasso$finalModel, xvar = "penalty", use.color = TRUE)
mod_lasso$finalModel
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
download.file(url, destfile="gaData.csv", mode="wb")
dat <- read.csv("gaData.csv")
library(lubridate)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
?ts
?bats
?forecast
library(e1071)
?svm
setwd("C:/Data Science/8_Practical_Machine_Learning")
knitr::opts_chunk$set(echo = TRUE)
if(!file.exists("./data")){dir.create("./data")}
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url_train, destfile="./data/pml-training.csv")
download.file(url_test, destfile="./data/pml-testing.csv")
training <- read.csv("pml-training.csv")
training <- read.csv("./data/pml-training.csv")
testing <- read.csv("./data/pml-testing.csv")
dim(training)
dim(testing)
testing
names(training)
dim(training)
dim(testing)
original_training <- read.csv("./data/pml-training.csv")
original_testing <- read.csv("./data/pml-testing.csv")
library(caret);
set.seed(31413)
inTrain <- createDataPartition(y=original_training$classe, p=0.75, list=FALSE)
training <- original_training[inTrain,]
testing <- original_training[-inTrain,]
dim(training)
dim(testing)
names(training)
names(original_training)
str(original_training)
inTrain <- createDataPartition(y=original_training$classe, p=0.7, list=FALSE)
training <- original_training[inTrain,]
testing <- original_training[-inTrain,]
dim(training)
dim(testing)
library(ggplot2)
length(training)
ncol(training)
for (i in 1:ncol(training)){
percentagena <- sum(is.na(training[,i]/nrow(training)))
}
head(percentagena)
percentagena
original_training <- read.csv("./data/pml-training.csv", na.strings=c("","NA","#DIV/0!"))
original_testing <- read.csv("./data/pml-testing.csv",na.strings=c("","NA","#DIV/0!"))
inTrain <- createDataPartition(y=original_training$classe, p=0.7, list=FALSE)
training <- original_training[inTrain,]
testing <- original_training[-inTrain,]
for (i in 1:ncol(training)){
percentagena[,i] <- sum(is.na(training[,i]/nrow(training)))
}
for (i in 1:ncol(training)){
percentagena[1,i] <- sum(is.na(training[,i]/nrow(training)))
}
sum(is.na(training[,1]/nrow(training)))
sum(is.na(training[,2]/nrow(training)))
for (i in 1:ncol(training)){
percentagena[,1] <- sum(is.na(training[,1]/nrow(training)))
}
percentagena[,1] <- sum(is.na(training[,1]/nrow(training)))
frac_na <- sum(is.na(training[,1]/nrow(training)))
frac_na[,i] <- data.frame(sum(is.na(training[,i]/nrow(training)))
)
for (i in 1:ncol(training)){
frac_na[,i] <- sum(is.na(training[,i]/nrow(training))
}
for (i in 1:ncol(training)){
frac_na[,i] <- sum(is.na(training[,i]/nrow(training)))
}
for (i in 1:ncol(training)){
frac_na <- sum(is.na(training[,i]/nrow(training)))
}
frac_na
frac_na[,2]
dim(frac_na)
for (i in 1:ncol(training)){
frac_na <- data.frame(matrix(sum(is.na(training[,i]/nrow(training)))))
}
head(frac_na)
for (i in 1:ncol(training)){
frac_na <- data.frame(matrix(sum(is.na(training[,i]/nrow(training)))),ncol=ncol(training))
}
head(frac_na)
frac_na <- colSums(is.na(training))
head(frac_na)
dim(frac_na)
frac_na
class(frac_na)
frac_na <- colSums(is.na(training))/nrow(training)
frac_na
frac_na <- data.frame(colSums(is.na(training))/nrow(training))
frac_na
?data.frame
frac_na <- data.frame(colSums(is.na(training))/nrow(training),header=FALSE)
frac_na
frac_na <- data.frame(frac=colSums(is.na(training))/nrow(training))
frac_na
class(frac_na)
class(frac_na$frac)
qplot(frac,data=frac_na)
qplot(frac,data=frac_na,xlab="Fraction of NAs")
qplot(frac,data=frac_na,xlab="Fraction of NAs")
for (i in 1:ncol(training)) {
if (sum(is.na(training[,i]/nrow(training) >= 0.9))){
training_clean <- training[,-i]
}
}
dim(training_clean)
?grep
grep(frac_na$frac >= 0.9)
frac_na$frac >= 0.9
as.numeric(frac_na$frac <= 0.9)
frac_na$frac <= 0.9
grep(frac_na$frac <=0.9, TRUE)
egrep(frac_na$frac <=0.9, TRUE)
grepl(frac_na$frac <=0.9, TRUE)
which(frac_na$frac <= 0.9)
training_clean <- training[,-colno]
colno <- which(frac_na$frac <= 0.9)
training_clean <- training[,-colno]
dim(training_clean)
training_clean <- training[,colno]
dim(training_clean)
head(training_clean)
colno <- which(frac_na$frac >= 0.9)
training_clean <- training[,-colno]
dim(training_clean)
training_clean
training_clean <- training[,-1:7]
training_clean <- training[,-(1:7)]
names(training_clean)
training_clean <- training[,-colno]
training_clean <- training_clean[,-(1:7)]
dim(training_clean)
nearZeroVar(training_clean)
?nearZeroVar
nearZeroVar(training_clean,saveMetrics=TRUE)
library(randomForest)
testing_clean <- testing[,-colno]
testing_clean <- testing_clean[,-(1:7)]
original_testing_clean <- original_testing[,-colno]
original_testing_clean <- original_testing_clean[,-(1:7)]
dim(testing_clean)
dim(original_testing_clean)
?predict
modFit_rf <- train(classe ~., data=training_clean,method="rf")
modFit_rf
set.seed(647)
modFit_rf <- train(classe ~., data=training_clean,method="rf") ##Random forest
set.seed(31413)
original_training <- read.csv("./data/pml-training.csv", na.strings=c("","NA","#DIV/0!"))
original_testing <- read.csv("./data/pml-testing.csv",na.strings=c("","NA","#DIV/0!"))
inTrain <- createDataPartition(y=original_training$classe, p=0.6, list=FALSE)
training <- original_training[inTrain,]
testing <- original_training[-inTrain,]
frac_na <- data.frame(frac=colSums(is.na(training))/nrow(training))
qplot(frac,data=frac_na,xlab="Fraction of NAs")
colno <- which(frac_na$frac >= 0.9)
training_clean <- training[,-colno]
training_clean <- training_clean[,-(1:7)]
nearZeroVar(training_clean,saveMetrics=TRUE)
dim(training_clean)
testing_clean <- testing[,-colno]
testing_clean <- testing_clean[,-(1:7)]
original_testing_clean <- original_testing[,-colno]
original_testing_clean <- original_testing_clean[,-(1:7)]
set.seed(647)
modFit_rf <- train(classe ~., data=training_clean,method="rf") ##Random forest
